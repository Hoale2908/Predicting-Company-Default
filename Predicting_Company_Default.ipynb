{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6mXK2tI8gjc"
      },
      "source": [
        "# **1. Project Overview**\n",
        "**Goal:**   Develop a model to predict a company's probability of default (PD) based on its financial ratios.\n",
        "\n",
        "**Data:**    \n",
        "*   Historical financial ratios of public companies during the 1995-2004 period, and\n",
        "*   Historical default events of public companies during the 1995-2004 period.\n",
        "\n",
        "**Features Definition:**\n",
        "* WC2TA: Working capital/Total assets\n",
        "* RE2TA: Retained earning/Total assets\n",
        "* EBIT2TA: EBIT/Total Assets\n",
        "* ME2TL: Market Equity/Total Liabilities\n",
        "* S2TA: Sales/Total assets\n",
        "* Default: 1 for defaut events, 0 for non-default events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRG_llzC8CkI"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff7luryRpKV_"
      },
      "source": [
        "# **2. Data Quality & EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0PVlq8nSV75"
      },
      "source": [
        "Exploratory Data Analysis is presented at the end of this notebook to keep the focus on developing and validating the models. Below are initial findings about the data:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVsX1JkmE8w5"
      },
      "source": [
        "* There are 3900 rows of data for 830 companies, each of which is represented by one ID.\n",
        "* There is no missing data.\n",
        "* The dataset is significantly imbalanced with default events accounts for only 2% of the dataset.\n",
        "* We will change the data type of \"Year\" and \"Default\" to *datetime* and *categorical*, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qmjz1Z-AH_c"
      },
      "outputs": [],
      "source": [
        "# Importing the data\n",
        "comp_data = pd.read_table(\"/Users/hoale/Code/Predicting PD by financial ratios/comp_data.txt\", sep=\"|\")\n",
        "comp_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzT7ad25D6qC",
        "outputId": "f568bd1c-0a11-45c7-e7f2-84dc8bcb323e"
      },
      "outputs": [],
      "source": [
        "comp_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comp_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(comp_data['Default'].value_counts())\n",
        "print('Percentage of Default (%):', len(comp_data[comp_data['Default']==1])/3900*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kULZxDAbIBI4",
        "outputId": "fcc68e27-792a-4535-d89a-fd1fb94695c5"
      },
      "outputs": [],
      "source": [
        "# Convert the features' data types\n",
        "comp_data['Default'] = comp_data['Default'].astype('category')\n",
        "comp_data['Year'] = pd.to_datetime(comp_data['Year'], format=\"%Y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC0AQkheaJxL"
      },
      "source": [
        "# **3. Model Development & Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4WuirjznujH"
      },
      "source": [
        "## *1 - Logistic Regression*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1zG1XHVMQHA"
      },
      "outputs": [],
      "source": [
        "# Assigning the features and target\n",
        "X = comp_data[['WC2TA','RE2TA','EBIT2TA','ME2TL','S2TA']]\n",
        "y = comp_data[['Default']]\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, precision_recall_curve\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuG5fpjX3DSF"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset to train and test subsets\n",
        "random_state = 12\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = random_state, stratify = y)\n",
        "\n",
        "# Using Logistic Regression to fit the model on the train dataset\n",
        "lgr = LogisticRegression(solver='lbfgs')\n",
        "lgr = lgr.fit(X_train, np.ravel(y_train))\n",
        "\n",
        "# Predicting the probabilities of default\n",
        "preds = lgr.predict_proba(X_test)\n",
        "preds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\n",
        "\n",
        "# Trying different thresholds from 0% to 90% with an increment of 5%\n",
        "thresholds = np.arange(0.0, 0.95, 0.05)\n",
        "default_recall = []\n",
        "nondefault_recall = []\n",
        "\n",
        "def threshold_assign(i, col_name):\n",
        "    preds_df[col_name] = preds_df['prob_default'].apply(lambda x: 1 if x > i else 0)\n",
        "\n",
        "for i in thresholds:\n",
        "    col_name = \"prd_\" + str(i)\n",
        "    threshold_assign(i, col_name)\n",
        "    def_recall = precision_recall_fscore_support(y_test, preds_df[col_name])[0][1]\n",
        "    default_recall.append(def_recall)\n",
        "    nondef_recall = precision_recall_fscore_support(y_test, preds_df[col_name])[1][1]\n",
        "    nondefault_recall.append(nondef_recall)\n",
        "\n",
        "# Plotting the default recall and nondefault recalls\n",
        "plt.figure(figsize = [12,3])\n",
        "plt.plot(thresholds, default_recall)\n",
        "plt.plot(thresholds, nondefault_recall)\n",
        "plt.xlabel('Probability Threshold')\n",
        "plt.xticks(thresholds)\n",
        "plt.legend(['Non-default recall', \"Default recall\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Using a test size of 0.5, the Default recall decreases significantly at the 5% and 10% thresholds, after which it continues to decrease slowly. \n",
        "* The Non-default recall peaks at the threshold of 40% - 45%, then also decreases.\n",
        "* Since there is no information regarding the Loan Value, LGD, or lender's profit, we cannot calculate and use Expected Loss as a way to choose the most appropriate threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnME7uDgBMwg"
      },
      "source": [
        "## *2 - Gradient Boosted Tree*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2NFdyJtKA6a"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = random_state, stratify = y)\n",
        "\n",
        "# Train a baseline GBT model with all features and make predictions\n",
        "# !pip install xgboost\n",
        "import xgboost as xgb\n",
        "gbt = xgb.XGBClassifier()\n",
        "gbt.fit(X_train, np.ravel(y_train))\n",
        "gbt_preds = gbt.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, gbt_preds, labels=['Non-default', 'Default'],))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The classification report shows all zero scores when we fit a simple XGBoost model (without any feature engineering, model tuning, or data preprocessing).\n",
        "Next, we will try to adjust the threshold to see if we can get a better model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adjust the threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, gbt_preds)\n",
        "\n",
        "# Find the optimal threshold\n",
        "optimal_idx = np.argmax(precision + recall)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "# Predict with the optimal threshold\n",
        "gbt_pred_proba = gbt.predict_proba(X_test)[:, 1]\n",
        "gbt_pred_optimal = (gbt_pred_proba >= optimal_threshold)\n",
        "\n",
        "# Evaluate the model\n",
        "print('Optimal threshold: ', optimal_threshold)\n",
        "print(classification_report(y_test, gbt_pred_optimal))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Default recall of 1 and F1 score of 0.04 suggest a low performance. Also, the optimal threshold is found at 0. All companies therefore will be predicted as *Default* even when their probability of default is just 1%. It is because the dataset is highly imbalanced. \n",
        "\n",
        "We will use oversampling technique to overcome the class imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=12)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the model on the resampled dataset\n",
        "rsp_gbt = xgb.XGBClassifier(enable_categorical=True)\n",
        "rsp_gbt.fit(X_resampled, np.ravel(y_resampled))\n",
        "rsp_gbt_preds = rsp_gbt.predict(X_test)\n",
        "rsp_pred_proba = rsp_gbt.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Classification report and AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(classification_report(y_test, rsp_gbt_preds))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, rsp_pred_proba))\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, rsp_pred_proba)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have a better model, demonstrated by a AUC of 0.9238, and accuracy of 0.97 as compared to 0.02 previously. We can also apply this technique to a Logistic Regression model to see if we could improve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Train a simple logistic regression model\n",
        "rsp_log_model = LogisticRegression()\n",
        "rsp_log_model.fit(X_resampled, np.ravel(y_resampled))\n",
        "\n",
        "# Predict on the test set\n",
        "rsp_log_preds = rsp_log_model.predict(X_test)\n",
        "rsp_log_proba = rsp_log_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, rsp_log_preds))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, rsp_log_proba))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although its performance has been improved (accuracy increased from x to 0.87), the Linear Regression model is still not as good as the Gradient Boosted one. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **4. Conclusion**\n",
        "\n",
        "We have developed a model to predict whether a company will default based on its financial ratios. \n",
        "- The model uses Gradient Boosting technique and is trained on a dataset of finanial ratios of 830 companies published over the period from 1995 through 2004. \n",
        "- The model has correct prediction 97% of the time including both Default and Non-default events.\n",
        "\n",
        "# **5. Potential improvements**\n",
        "\n",
        "A model with 97% accuracy is quite impressive. However, low precision is still a problem since the associated financial costs could be disastrous. We have some options below when looking to improve our model or develop alternatives:\n",
        "\n",
        "- Feature engineering\n",
        "- Parameter tuning\n",
        "- Other modeling techniques, e.g. Random Forest, Gradient Boosting Machines, LightGBM, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unQWyoffpTlM"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hucw363kX36i"
      },
      "source": [
        "### WC2TA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comp_data.boxplot('WC2TA', by='Default', grid=True, vert=False, meanline=True, showmeans=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variance in WC2TA between default and non-default observations is not significant. \\\n",
        "However, default companies tends to have WC2TA closer to 0. \\\n",
        "Most outliers with WC2TA of less than -0.25 or higher than 0.5 are non-defaut companies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RE2TA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comp_data.boxplot('RE2TA', by='Default', grid=True, vert=False, meanline=True, showmeans=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The difference is quite clear with RE2TA. Default companies tend to have a lower RE2TA, with nearly 75% even have negative Retained Earning. Most of non-default companies have a positive RE2TA. However, non-default companies have a noticably wide range of RE2TA. There are many outliers with RE2TA lower than that of default companies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EBIT2TA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comp_data.boxplot('EBIT2TA', by='Default', grid=True, vert=False, meanline=True, showmeans=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The difference in EBIT2TA is not significant. We can try to have a closer look at the range -0.1 to 0.1 where there are fewer outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ebitta = comp_data[(comp_data['EBIT2TA'] < 0.1) & (comp_data['EBIT2TA'] > -0.1)]\n",
        "df_ebitta.boxplot('EBIT2TA', by='Default', grid=True, vert=False, meanline=True, showmeans=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can see that non-default companies have a slightly higher EBIT2TA on average, with 50% of those are around 4-6%. Meanwhile, 50% of default companies have EBIT2TA in the range of 2-5%. The difference is however insignificant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ME2TL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comp_data.boxplot('ME2TL', by='Default', grid=True, vert=False, showfliers=True, showmeans=True, meanline=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non-default companies usually have higher ME2TL. And if we remove observations where ME2TL > 4, we will have the following boxplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = comp_data[comp_data['ME2TL']<4]\n",
        "df1.boxplot('ME2TL', by='Default', grid=True, vert=False, showfliers=True, showmeans=True, meanline=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it's easier to see that default companies have much lower ME2TL. Most of them have a ME2TL of lower than 0.4, whereas more than 75% of non-default companies have a ME2TL of higher than 0.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### S2TA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comp_data.boxplot('S2TA', by='Default', grid=True, vert=False, showfliers=True, showmeans=True, meanline=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's remove some outliers with S2TA larger than 0.8 to see if we can get a better insight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_s2ta = comp_data[comp_data['S2TA'] < 0.8]\n",
        "df_s2ta.boxplot('S2TA', by='Default', grid=True, vert=False, showfliers=True, showmeans=True, meanline=True, figsize=(12,2))\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oftentimes we think that companies with higher sales are doing better, and that's only generally true if holding everything else the same. But since we are looking at Sales/Total assets, it is not sufficient to comment on the company's ability to pay off its debt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlations among features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr = comp_data.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(corr, mask = mask, cmap=cmap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The highest pair-wise correlation is 0.31, observed between EBIT2TA and RE2TA. \n",
        "* Surprisingly, S2TA has very low correlations with RE2TA and ME2TL."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
